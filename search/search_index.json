{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udfe0 RAG-VisualRec","text":"<p>This repository contains a resource of an open resource for vision and text-enhanced Retrieval-Augmented Generation (RAG) in the recommendation domain. RAG-VisualRec provides a new, reproducible test\u2011bed for multimodal RAG research. It is designed as a transparent, modular, and extensible resource for rigorous multimodal recommendation research, with a primary goal of bridging the gap between theoretical advances (e.g., fusion techniques, textual data augmentation, multi-modal retrieval, augmented generation) and practical, reproducible workflows that any researcher can adapt or extend.</p> <p></p>"},{"location":"#architecture","title":"\ud83e\udde0 Architecture","text":"<p>The overall pipeline of RAG-VisualRec contains below steps:</p> <ul> <li>Data preparation and Ingestion, starting with MovieLens (<code>latest-small</code> and <code>1M</code>) dataset</li> <li>Multimodal embedding extraction (including textual, visual, and audio)</li> <li>Applying fusion strategies (e.g., concatenation, PCA, CCA)</li> <li>Applying user embedding construction (random, averag, or temporal)</li> <li>Candidate retrieval</li> <li>Profile augmentation and LLM prompting (manual or LLM-based)</li> <li>Evaluation and logging (accuracy, beyond-accuracy, fairness/robustness)</li> </ul>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>The whole framework is fully configurable through a centralized parameter block in the Google Colab notebook. Check the <code>codes</code> directory for more information.</p>"},{"location":"#citation","title":"\ud83d\udcda Citation","text":"<p>This research is submitted to ACM Transactions on Recommender Systems.</p> <pre><code>@article{tourani2025rag,\n  title={RAG-VisualRec: An Open Resource for Vision-and Text-Enhanced Retrieval-Augmented Generation in Recommendation},\n  author={Tourani, Ali and Nazary, Fatemeh and Deldjoo, Yashar},\n  journal={arXiv preprint arXiv:2506.20817},\n  year={2025}\n  doi={https://doi.org/10.48550/arXiv.2506.20817}\n}\n</code></pre>"},{"location":"#related-repositories","title":"\ud83d\udcce Related Repositories","text":"<ul> <li>\ud83d\udd28 ViLLA-MMBench: Multimodal Recommender</li> <li>\ud83c\udf9e\ufe0f MoViFex: Movie Recommendation Framework</li> </ul>"},{"location":"#license","title":"\ud83d\udd11 License","text":"<p>This project is licensed under the GPL-3.0 license - see the LICENSE for more details.</p>"},{"location":"#contact","title":"\ud83d\udcec Contact","text":"<p>If you have any questions or collaboration opportunities, please open an issue or contact the authors.</p>"},{"location":"codes/","title":"\ud83d\udcbb Using the Framework","text":"<p>Here you can find Google Colab notebooks related to RAG-VisualRec in the below:</p>"},{"location":"codes/#pipeline","title":"\ud83d\ude80 Pipeline","text":"<ul> <li>RAG-VisualRec</li> </ul>"},{"location":"codes/#evaluation-and-visualization","title":"\ud83d\udcca Evaluation and Visualization","text":"<ul> <li>Bar Chart - Ablation Study</li> <li>Radar Chart - Modality Impact</li> </ul>"},{"location":"configs/","title":"\u2699\ufe0f Configuration Schema","text":"<p>The pipeline is fully configurable through a single config block. This makes it easy to try out different experiments without changing the code \u2014 just edit the config and rerun.</p>"},{"location":"configs/#main-parameters","title":"\ud83d\udd11 Main Parameters","text":""},{"location":"configs/#dataset","title":"\ud83d\udcc2 Dataset","text":"<ul> <li><code>ml_latest_small</code> \u2192 MovieLens Latest Small</li> <li><code>lfm360k</code> \u2192 Last.fm 360K</li> </ul>"},{"location":"configs/#embeddings","title":"\ud83e\udde9 Embeddings","text":"<ul> <li><code>textual</code> \u2192 Text-only embeddings</li> <li><code>visual</code> \u2192 Visual-only embeddings</li> <li><code>audio</code> \u2192 Audio-only embeddings</li> <li><code>fused_concat</code> \u2192 Concatenate all modalities</li> <li><code>fused_pca</code> \u2192 PCA-based fusion (128-dim)</li> <li><code>fused_cca</code> \u2192 CCA-based fusion (64-dim)</li> <li><code>fused_avg</code> \u2192 Average embeddings</li> </ul>"},{"location":"configs/#llm-model","title":"\ud83e\udd16 LLM Model","text":"<ul> <li><code>openai</code> \u2192 OpenAI Ada embeddings</li> <li><code>sentence_transformer</code> \u2192 MiniLM or similar</li> <li><code>llama3</code> \u2192 HuggingFace LLaMA model</li> </ul>"},{"location":"configs/#user-vector-strategy","title":"\ud83d\udc64 User Vector Strategy","text":"<ul> <li><code>random</code> \u2192 Random baseline (sanity check)</li> <li><code>average</code> \u2192 Average of liked item embeddings</li> <li><code>temporal</code> \u2192 Weighted by recency of interactions</li> </ul>"},{"location":"configs/#retrieval","title":"\ud83d\udd0e Retrieval","text":"<ul> <li><code>N</code> \u2192 Number of nearest neighbors (default: <code>50</code>)</li> </ul>"},{"location":"configs/#recommendation","title":"\ud83c\udfac Recommendation","text":"<ul> <li><code>K</code> \u2192 Final top recommendations (default: <code>10</code>)</li> <li><code>explainable</code> \u2192 <code>true/false</code> (include reasoning or not)</li> </ul>"},{"location":"configs/#runtime-settings","title":"\u26a1 Runtime Settings","text":"<ul> <li><code>use_gpu</code> \u2192 Use GPU acceleration if available</li> <li><code>seed</code> \u2192 Random seed for reproducibility</li> <li><code>batch_size</code> \u2192 Batch size for embedding &amp; retrieval</li> </ul>"},{"location":"configs/#example-config-block","title":"\ud83d\udee0 Example Config Block","text":"<pre><code>dataset: ml_latest_small\nembeddings: fused_pca\nllm_model: sentence_transformer\nuser_vector: temporal\nretrieval:\n  N: 50\nrecommendation:\n  K: 10\n  explainable: true\nruntime:\n  use_gpu: true\n  seed: 42\n  batch_size: 64\n</code></pre>"},{"location":"configs/#notes","title":"\u2705 Notes","text":"<ul> <li>Default values are set to match the benchmarks used in experiments.</li> <li>Changing any config parameter requires no code edits \u2014 just update the block.</li> <li>This makes the system flexible for rapid experimentation and extensibility.</li> </ul>"},{"location":"pipeline/","title":"\ud83d\udc63 Pipeline Walkthrough","text":""},{"location":"pipeline/#1-data-preparation-and-ingestion","title":"1\ufe0f\u20e3 Data Preparation and Ingestion","text":"<p>We start by loading data from MovieLens (small or 1M).</p> <ul> <li>User\u2013item interactions are cleaned and indexed.</li> <li>Metadata like title, genres, and tags is merged in.</li> <li>Missing information is handled smoothly (important for cold-start or incomplete data).</li> </ul> <p>\ud83d\udc49 Example:</p> <ul> <li>Input: <code>Nixon (1995)</code> \u2192 Genres: Drama, Biography \u2192 Description: (missing)</li> <li>Output (after LLM enrichment):   \u201cNixon (1995) explores the troubled psyche and political career of America's 37th president...\u201d</li> </ul> <p>We also tag each movie by popularity:</p> <ul> <li>Head (top 10%)</li> <li>Mid-tail (next 40%)</li> <li>Long-tail (bottom 50%)</li> </ul>"},{"location":"pipeline/#2-multimodal-embedding-extraction","title":"2\ufe0f\u20e3 Multimodal Embedding Extraction","text":"<p>For each item, we generate embeddings (vector representations) from different sources:</p> <ul> <li>Text \u2192 From movie descriptions using models like OpenAI Ada, MiniLM, or LLaMA.</li> <li>Visual \u2192 From trailer/keyframes using ResNet-50 (2048-dim vectors).</li> <li>Audio (optional) \u2192 Extract MFCC features (128-dim).</li> <li> <p>Fusion \u2192 Combine modalities using methods like:</p> </li> <li> <p>Concat (stack vectors)</p> </li> <li>PCA (reduce to 128-dim)</li> <li>CCA (align text &amp; visual spaces)</li> <li>Avg (simple average)</li> </ul> <p>\ud83d\udc49 Example:</p> <ul> <li>Text embedding: <code>[... -0.31, 0.54, 1.02 ...]</code></li> <li>Visual embedding: <code>[... 0.11, -0.22, 0.91 ...]</code></li> <li>CCA fusion \u2192 joint 64-dim vector: <code>[... 0.44, 0.08, -0.32 ...]</code></li> </ul>"},{"location":"pipeline/#3-embedding-swap-re-embedding","title":"3\ufe0f\u20e3 Embedding Swap &amp; Re-embedding","text":"<p>If movie info changes (e.g., after augmentation), we re-run the embedding step. \u2705 Ensures all experiments always use up-to-date representations.</p>"},{"location":"pipeline/#4-user-embedding-construction","title":"4\ufe0f\u20e3 User Embedding Construction","text":"<p>We create user profiles in the same space as items. Options:</p> <ul> <li>Random \u2192 sanity check baseline.</li> <li>Average \u2192 mean of all liked items.</li> <li>Temporal \u2192 recent interactions get more weight.</li> </ul> <p>\ud83d\udc49 Example: User 42 watched Nixon (1995), The Post, Frost/Nixon.</p> <ul> <li>More recent movies (e.g., The Post) count more in their profile.</li> </ul>"},{"location":"pipeline/#5-candidate-retrieval","title":"5\ufe0f\u20e3 Candidate Retrieval","text":"<p>With user embeddings ready:</p> <ul> <li>We build a kNN index of all items.</li> <li>For each user \u2192 find top-N most similar items.</li> </ul> <p>This step reflects all upstream choices (embedding model, fusion method, user strategy).</p>"},{"location":"pipeline/#6-profile-augmentation-llm-prompting","title":"6\ufe0f\u20e3 Profile Augmentation &amp; LLM Prompting","text":"<p>We enhance user profiles with structured info:</p> <ul> <li>Manual: Extracted from history (genres, top movies, tags).</li> <li>LLM-based: Generated with a short natural-language summary.</li> </ul> <p>\ud83d\udc49 Example Profile:</p> <pre><code>{\n  \"Genres\": [\"Drama\", \"Biography\"],\n  \"Top items\": [\"Nixon (1995)\", \"The Post\"],\n  \"Taste\": \"Prefers political dramas exploring real historical events.\"\n}\n</code></pre> <p>This profile + candidate movies \u2192 passed to the LLM with instructions.</p>"},{"location":"pipeline/#7-llm-re-ranking","title":"7\ufe0f\u20e3 LLM Re-ranking","text":"<p>The LLM receives:</p> <ol> <li>User profile</li> <li>Candidate movies</li> <li>Instructions</li> </ol> <p>It outputs a ranked list.</p> <ul> <li>ID-only mode \u2192 Just movie IDs (privacy-friendly).</li> <li>Explainable mode \u2192 IDs + reasoning.</li> </ul> <p>If parsing fails \u2192 fallback to kNN list.</p>"},{"location":"pipeline/#8-evaluation-logging","title":"8\ufe0f\u20e3 Evaluation &amp; Logging","text":"<p>Every run is measured using:</p> <ul> <li>Accuracy \u2192 Recall\\@K, nDCG\\@K, MAP, MRR.</li> <li>Beyond-accuracy \u2192 Coverage, novelty, diversity, long-tail %.</li> <li>Fairness/robustness \u2192 Cold-start, exposure balance.</li> </ul> <p>Results are logged per user, averaged, and exported (CSV/Parquet). \u2705 Intermediate artifacts (embeddings, candidates, logs) are checkpointed for reproducibility.</p>"}]}